{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary packages from Keras Applications API \n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import sys\n",
    "import keras\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "set_seed = 42\n",
    "num_classes = 6\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "patience_epochs = 5\n",
    "train_val_dir = './split_smoking_images/train/'\n",
    "test_dir = './split_smoking_images/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base pre-trained model, without top dense layers\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "# We can see all the layers: \n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "# Add a global spatial average pooling layer to reduce dimensionality.\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# Add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# And a logistic layer of width num_classes \n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "# Complete model using Model object\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "# first: train only the top layers (randomly initialized)\n",
    "# i.e. freeze all convolutional MobileNetV2 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2801 images belonging to 6 classes.\n",
      "Found 697 images belonging to 6 classes.\n",
      "{'beer_bottle': 0, 'beer_glass': 1, 'grocery_store': 2, 'library': 3, 'pew': 4, 'tobacco_store': 5}\n"
     ]
    }
   ],
   "source": [
    "train_val_datagen = ImageDataGenerator(rotation_range=20, zoom_range=0.15, rescale=1./255,\n",
    "                         width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\",\n",
    "                         validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_val_datagen.flow_from_directory(\n",
    "    train_val_dir, batch_size=batch_size, shuffle=True, seed=set_seed,\n",
    "    subset='training') # set as training data to seperate from validation!\n",
    "\n",
    "val_generator = train_val_datagen.flow_from_directory(\n",
    "    train_val_dir, batch_size=batch_size, shuffle=True, seed=set_seed,\n",
    "    subset='validation') # set as validation data to seperate from training!\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=patience_epochs)\n",
    "checkpoint_callback = ModelCheckpoint('mobilenetv2'+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "175/175 [==============================] - 51s 290ms/step - loss: 0.8507 - acc: 0.7029 - val_loss: 0.5491 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54910, saving model to mobilenetv2.h5\n",
      "Epoch 2/40\n",
      "175/175 [==============================] - 47s 270ms/step - loss: 0.5912 - acc: 0.7939 - val_loss: 0.4852 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54910 to 0.48524, saving model to mobilenetv2.h5\n",
      "Epoch 3/40\n",
      "175/175 [==============================] - 50s 285ms/step - loss: 0.5231 - acc: 0.8211 - val_loss: 0.6573 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48524\n",
      "Epoch 4/40\n",
      "175/175 [==============================] - 50s 284ms/step - loss: 0.5170 - acc: 0.8129 - val_loss: 0.4670 - val_acc: 0.8326\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48524 to 0.46700, saving model to mobilenetv2.h5\n",
      "Epoch 5/40\n",
      "175/175 [==============================] - 52s 297ms/step - loss: 0.5014 - acc: 0.8250 - val_loss: 0.4373 - val_acc: 0.8502\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46700 to 0.43730, saving model to mobilenetv2.h5\n",
      "Epoch 6/40\n",
      "175/175 [==============================] - 50s 285ms/step - loss: 0.4902 - acc: 0.8243 - val_loss: 0.6222 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43730\n",
      "Epoch 7/40\n",
      "175/175 [==============================] - 51s 291ms/step - loss: 0.4775 - acc: 0.8236 - val_loss: 0.4574 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43730\n",
      "Epoch 8/40\n",
      "175/175 [==============================] - 50s 283ms/step - loss: 0.4654 - acc: 0.8297 - val_loss: 0.6342 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43730\n",
      "Epoch 9/40\n",
      "175/175 [==============================] - 53s 301ms/step - loss: 0.4263 - acc: 0.8450 - val_loss: 0.5763 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43730\n",
      "Epoch 10/40\n",
      "175/175 [==============================] - 51s 289ms/step - loss: 0.4589 - acc: 0.8336 - val_loss: 0.5107 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10843a2ba8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = val_generator, \n",
    "    validation_steps = val_generator.samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback], verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 880 images belonging to 6 classes.\n",
      "2/2 [==============================] - 14s 7s/step\n"
     ]
    }
   ],
   "source": [
    "# Make a final data generator, for evaluating.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = train_val_datagen.flow_from_directory(\n",
    "    test_dir, batch_size=400, shuffle=False) \n",
    "\n",
    "loss, acc = model.evaluate_generator(test_generator, verbose=1, steps = 2,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is 0.4353136122226715\n",
      "The test accuracy is 0.8537499904632568\n"
     ]
    }
   ],
   "source": [
    "print(\"The test loss is \" + str(loss))\n",
    "print(\"The test accuracy is \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 Conv1_pad\n",
      "2 Conv1\n",
      "3 bn_Conv1\n",
      "4 Conv1_relu\n",
      "5 expanded_conv_depthwise\n",
      "6 expanded_conv_depthwise_BN\n",
      "7 expanded_conv_depthwise_relu\n",
      "8 expanded_conv_project\n",
      "9 expanded_conv_project_BN\n",
      "10 block_1_expand\n",
      "11 block_1_expand_BN\n",
      "12 block_1_expand_relu\n",
      "13 block_1_pad\n",
      "14 block_1_depthwise\n",
      "15 block_1_depthwise_BN\n",
      "16 block_1_depthwise_relu\n",
      "17 block_1_project\n",
      "18 block_1_project_BN\n",
      "19 block_2_expand\n",
      "20 block_2_expand_BN\n",
      "21 block_2_expand_relu\n",
      "22 block_2_depthwise\n",
      "23 block_2_depthwise_BN\n",
      "24 block_2_depthwise_relu\n",
      "25 block_2_project\n",
      "26 block_2_project_BN\n",
      "27 block_2_add\n",
      "28 block_3_expand\n",
      "29 block_3_expand_BN\n",
      "30 block_3_expand_relu\n",
      "31 block_3_pad\n",
      "32 block_3_depthwise\n",
      "33 block_3_depthwise_BN\n",
      "34 block_3_depthwise_relu\n",
      "35 block_3_project\n",
      "36 block_3_project_BN\n",
      "37 block_4_expand\n",
      "38 block_4_expand_BN\n",
      "39 block_4_expand_relu\n",
      "40 block_4_depthwise\n",
      "41 block_4_depthwise_BN\n",
      "42 block_4_depthwise_relu\n",
      "43 block_4_project\n",
      "44 block_4_project_BN\n",
      "45 block_4_add\n",
      "46 block_5_expand\n",
      "47 block_5_expand_BN\n",
      "48 block_5_expand_relu\n",
      "49 block_5_depthwise\n",
      "50 block_5_depthwise_BN\n",
      "51 block_5_depthwise_relu\n",
      "52 block_5_project\n",
      "53 block_5_project_BN\n",
      "54 block_5_add\n",
      "55 block_6_expand\n",
      "56 block_6_expand_BN\n",
      "57 block_6_expand_relu\n",
      "58 block_6_pad\n",
      "59 block_6_depthwise\n",
      "60 block_6_depthwise_BN\n",
      "61 block_6_depthwise_relu\n",
      "62 block_6_project\n",
      "63 block_6_project_BN\n",
      "64 block_7_expand\n",
      "65 block_7_expand_BN\n",
      "66 block_7_expand_relu\n",
      "67 block_7_depthwise\n",
      "68 block_7_depthwise_BN\n",
      "69 block_7_depthwise_relu\n",
      "70 block_7_project\n",
      "71 block_7_project_BN\n",
      "72 block_7_add\n",
      "73 block_8_expand\n",
      "74 block_8_expand_BN\n",
      "75 block_8_expand_relu\n",
      "76 block_8_depthwise\n",
      "77 block_8_depthwise_BN\n",
      "78 block_8_depthwise_relu\n",
      "79 block_8_project\n",
      "80 block_8_project_BN\n",
      "81 block_8_add\n",
      "82 block_9_expand\n",
      "83 block_9_expand_BN\n",
      "84 block_9_expand_relu\n",
      "85 block_9_depthwise\n",
      "86 block_9_depthwise_BN\n",
      "87 block_9_depthwise_relu\n",
      "88 block_9_project\n",
      "89 block_9_project_BN\n",
      "90 block_9_add\n",
      "91 block_10_expand\n",
      "92 block_10_expand_BN\n",
      "93 block_10_expand_relu\n",
      "94 block_10_depthwise\n",
      "95 block_10_depthwise_BN\n",
      "96 block_10_depthwise_relu\n",
      "97 block_10_project\n",
      "98 block_10_project_BN\n",
      "99 block_11_expand\n",
      "100 block_11_expand_BN\n",
      "101 block_11_expand_relu\n",
      "102 block_11_depthwise\n",
      "103 block_11_depthwise_BN\n",
      "104 block_11_depthwise_relu\n",
      "105 block_11_project\n",
      "106 block_11_project_BN\n",
      "107 block_11_add\n",
      "108 block_12_expand\n",
      "109 block_12_expand_BN\n",
      "110 block_12_expand_relu\n",
      "111 block_12_depthwise\n",
      "112 block_12_depthwise_BN\n",
      "113 block_12_depthwise_relu\n",
      "114 block_12_project\n",
      "115 block_12_project_BN\n",
      "116 block_12_add\n",
      "117 block_13_expand\n",
      "118 block_13_expand_BN\n",
      "119 block_13_expand_relu\n",
      "120 block_13_pad\n",
      "121 block_13_depthwise\n",
      "122 block_13_depthwise_BN\n",
      "123 block_13_depthwise_relu\n",
      "124 block_13_project\n",
      "125 block_13_project_BN\n",
      "126 block_14_expand\n",
      "127 block_14_expand_BN\n",
      "128 block_14_expand_relu\n",
      "129 block_14_depthwise\n",
      "130 block_14_depthwise_BN\n",
      "131 block_14_depthwise_relu\n",
      "132 block_14_project\n",
      "133 block_14_project_BN\n",
      "134 block_14_add\n",
      "135 block_15_expand\n",
      "136 block_15_expand_BN\n",
      "137 block_15_expand_relu\n",
      "138 block_15_depthwise\n",
      "139 block_15_depthwise_BN\n",
      "140 block_15_depthwise_relu\n",
      "141 block_15_project\n",
      "142 block_15_project_BN\n",
      "143 block_15_add\n",
      "144 block_16_expand\n",
      "145 block_16_expand_BN\n",
      "146 block_16_expand_relu\n",
      "147 block_16_depthwise\n",
      "148 block_16_depthwise_BN\n",
      "149 block_16_depthwise_relu\n",
      "150 block_16_project\n",
      "151 block_16_project_BN\n",
      "152 Conv_1\n",
      "153 Conv_1_bn\n",
      "154 out_relu\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 blocks, i.e. we will freeze\n",
    "# the first 135 layers and unfreeze the rest:\n",
    "for layer in model.layers[:144]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[144:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=patience_epochs)\n",
    "checkpoint_callback = ModelCheckpoint('mobilenetv2'+'.h6', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "175/175 [==============================] - 55s 313ms/step - loss: 0.7399 - acc: 0.7436 - val_loss: 1.3030 - val_acc: 0.7342\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.30303, saving model to mobilenetv2.h6\n",
      "Epoch 2/40\n",
      "175/175 [==============================] - 51s 289ms/step - loss: 0.5353 - acc: 0.8150 - val_loss: 1.1632 - val_acc: 0.7254\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.30303 to 1.16325, saving model to mobilenetv2.h6\n",
      "Epoch 3/40\n",
      "175/175 [==============================] - 51s 289ms/step - loss: 0.4551 - acc: 0.8425 - val_loss: 0.5828 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16325 to 0.58282, saving model to mobilenetv2.h6\n",
      "Epoch 4/40\n",
      "175/175 [==============================] - 51s 293ms/step - loss: 0.4381 - acc: 0.8497 - val_loss: 1.0170 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58282\n",
      "Epoch 5/40\n",
      "175/175 [==============================] - 51s 290ms/step - loss: 0.3813 - acc: 0.8636 - val_loss: 1.3777 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58282\n",
      "Epoch 6/40\n",
      "175/175 [==============================] - 50s 285ms/step - loss: 0.3668 - acc: 0.8811 - val_loss: 0.8305 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58282\n",
      "Epoch 7/40\n",
      "175/175 [==============================] - 51s 292ms/step - loss: 0.3428 - acc: 0.8818 - val_loss: 0.9962 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58282\n",
      "Epoch 8/40\n",
      "175/175 [==============================] - 51s 294ms/step - loss: 0.2954 - acc: 0.9022 - val_loss: 0.7943 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f108c25a6d8>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = val_generator, \n",
    "    validation_steps = val_generator.samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback], verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 12s 6s/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate_generator(test_generator, verbose=1, steps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is 0.8223159611225128\n",
      "The test accuracy is 0.8362500071525574\n"
     ]
    }
   ],
   "source": [
    "print(\"The test loss is \" + str(loss))\n",
    "print(\"The test accuracy is \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
